{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from math import floor\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.models import Model # Keras is the new high level API for TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version: 2.1.0\n",
      "Tensorflow Hub Version: 0.8.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Tensorflow Version: {}\\nTensorflow Hub Version: {}\".format(tf.__version__, hub.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/home/pablo/Desktop/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To list all subdirectories in directory 'test':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/pablo/Desktop/test/n00006484', '/home/pablo/Desktop/test/n00005787']\n"
     ]
    }
   ],
   "source": [
    "x = \"/home/pablo/Desktop/test/\"\n",
    "subdirs = [os.path.join(x, o) for o in os.listdir(x) if os.path.isdir(os.path.join(x,o))]\n",
    "print(subdirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/pablo/Desktop/test/n00006484\n",
      "/home/pablo/Desktop/test/n00005787\n"
     ]
    }
   ],
   "source": [
    "for dirs in subdirs:\n",
    "    print(dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/pablo/Desktop/test/n00006484\n",
      "/home/pablo/Desktop/test/n00005787\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "for i in np.arange(0, len(subdirs)):\n",
    "    print(subdirs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/pablo/Desktop/test/n00006484\n",
      "/home/pablo/Desktop/test/n00005787\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "filenamez=[]\n",
    "\n",
    "for i in np.arange(0, len(subdirs)):\n",
    "    names=os.listdir(subdirs[i])\n",
    "    filenamez.append(names)\n",
    "    print(subdirs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3171"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get all the full paths (not relative paths) for each item in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "new_dirs=[]\n",
    "files = []\n",
    "for dirs in subdirs:\n",
    "    # r=root, d=directories, f = files\n",
    "    for r, d, f in os.walk(dirs):\n",
    "        for file in f:\n",
    "            files.append(os.path.join(r, file))\n",
    "            new_dirs.append(os.path.split(dirs)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5984"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5984"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concat the lists into a dataframe for processing (next step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(zip(files, new_dirs)), columns=['metadata_file','target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Because all files except for the meta files have extensions Python recognizes, I'm dropping all extensions and keeping only unique values because these will be the JSON files (each file with an extension has an associated meta file). I'm doing this in a full dataframe so the class labels retain the same indices as the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/pablo/Desktop/test/n00006484/rp-841i5IwFoxM.jpg'"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.metadata_file[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the first index before the split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To list all files and their full paths (without extensions):\n",
    "#xyz=[]\n",
    "xyz=pd.Series()\n",
    "\n",
    "for i in np.arange(len(df)):\n",
    "    df['metadata_file'][i] = df['metadata_file'][i].split(\".\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/pablo/Desktop/test/n00006484/rp-841i5IwFoxM'"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.metadata_file[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5984"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3003"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.drop_duplicates())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop duplicates like mentioned above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3003"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metadata_file</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/pablo/Desktop/test/n00006484/aPkDH6Plnh1XHM</td>\n",
       "      <td>n00006484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/pablo/Desktop/test/n00006484/rp-841i5IwFoxM</td>\n",
       "      <td>n00006484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/pablo/Desktop/test/n00006484/Eud7D3RdzbTRAM</td>\n",
       "      <td>n00006484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/pablo/Desktop/test/n00006484/zi6RzWIEvqGCRM</td>\n",
       "      <td>n00006484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/pablo/Desktop/test/n00006484/j705FxfCA2CjCM</td>\n",
       "      <td>n00006484</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       metadata_file     target\n",
       "0  /home/pablo/Desktop/test/n00006484/aPkDH6Plnh1XHM  n00006484\n",
       "1  /home/pablo/Desktop/test/n00006484/rp-841i5IwFoxM  n00006484\n",
       "2  /home/pablo/Desktop/test/n00006484/Eud7D3RdzbTRAM  n00006484\n",
       "3  /home/pablo/Desktop/test/n00006484/zi6RzWIEvqGCRM  n00006484\n",
       "4  /home/pablo/Desktop/test/n00006484/j705FxfCA2CjCM  n00006484"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Pandas back to lists for faster processing with JSON reading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist = list(df.metadata_file)\n",
    "classlist = list(df.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the metadata filename list: (3003,)\n",
      "Shape of metadata target list: (3003,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of the metadata filename list: {}\\nShape of metadata target list: {}\".format(np.array(filelist).shape, np.array(classlist).shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions=[]\n",
    "ids=[]\n",
    "targets=[]\n",
    "pathz=[]\n",
    "new_dirs=[]\n",
    "titles=[]\n",
    "\n",
    "for file in filelist:\n",
    "    with open(file) as f:\n",
    "        json_data = json.load(f)\n",
    "        descriptions.append(json_data['description'])\n",
    "        ids.append(json_data['id'])\n",
    "        titles.append(json_data['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove all non-ascii characters (including foreign languages (there was a lot of japanese)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_titles = []\n",
    "new_descriptions = []\n",
    "\n",
    "# encode as ascii, decode to convert back to characters from bytes. Also, do regex cleanup on special characters not indicative of meaning\n",
    "for title in titles:\n",
    "    new_titles.append(re.sub(r\"[^a-zA-Z?.!,¿#@]+\", \" \",title.encode('ascii',errors='ignore').decode('UTF-8')))\n",
    "    \n",
    "for description in descriptions:\n",
    "    new_descriptions.append(re.sub(r\"[^a-zA-Z?.!,¿#@]+\", \" \",description.encode('ascii',errors='ignore').decode('UTF-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(new_titles)):\n",
    "    new_titles[i] = new_titles[i].lower()\n",
    "\n",
    "for i in range(len(new_descriptions)):\n",
    "    new_descriptions[i] = new_descriptions[i].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = pd.Series(df['target']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2998    n00005787\n",
       "2999    n00005787\n",
       "3000    n00005787\n",
       "3001    n00005787\n",
       "3002    n00005787\n",
       "Name: target, dtype: object"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2998    n00005787\n",
       "2999    n00005787\n",
       "3000    n00005787\n",
       "3001    n00005787\n",
       "3002    n00005787\n",
       "Name: target, dtype: object"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put words into a dictionary for downstream use\n",
    "import collections\n",
    "def build_dataset(words):\n",
    "    count = collections.Counter(words).most_common() #.most_common(100) to use the 100 most common words; .most_common() means zero is the most common\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return dictionary, reverse_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_train = new_titles[:floor(len(new_titles)*.75)]\n",
    "titles_test = new_titles[floor(len(new_titles)*.75):]\n",
    "\n",
    "descriptions_train = new_descriptions[:floor(len(new_descriptions)*.75)]\n",
    "descriptions_test = new_descriptions[floor(len(new_descriptions)*.75):]\n",
    "\n",
    "targets_train = targets[:floor(len(targets)*.75)]\n",
    "targets_test = targets[floor(len(targets)*.75):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title train length: 2252\n",
      "Description train length: 2252\n",
      "Target train length: 2252\n",
      "\n",
      "Title test length: 751\n",
      "Description test length: 751\n",
      "Target test length: 751\n"
     ]
    }
   ],
   "source": [
    "print(\"Title train length: {}\\nDescription train length: {}\\nTarget train length: {}\\n\\nTitle test length: {}\\nDescription test length: {}\\nTarget test length: {}\".format(len(titles_train), len(descriptions_train), len(targets_train), len(titles_test), len(descriptions_test), len(targets_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up Title Encoding (dictionarize for ALL titles (not just train or test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_word_list=[]\n",
    "\n",
    "for i in np.arange(len(new_titles)):\n",
    "    title_word_list.append(new_titles[i].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_flat_list = []\n",
    "\n",
    "for sublist in title_word_list:\n",
    "    for item in sublist:\n",
    "        title_flat_list.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_word_list = title_flat_list.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_words_unique = list(set(title_word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique train titles word count: 4601\n",
      "Unique validation titles word count: 4601\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique train titles word count: {}\\nUnique validation titles word count: {}\".format(len(title_words_unique), len(title_words_unique)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "title_word_to_id = Counter()\n",
    "\n",
    "for word in title_word_list:\n",
    "    title_word_to_id[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For unique dictionary values of key words\n",
    "title_word_to_id = {k:(i + 3) for i,(k,v) in enumerate(title_word_to_id.items())}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking to make sure the 10 most common words only appear once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 1),\n",
       " (4, 1),\n",
       " (5, 1),\n",
       " (6, 1),\n",
       " (7, 1),\n",
       " (8, 1),\n",
       " (9, 1),\n",
       " (10, 1),\n",
       " (11, 1),\n",
       " (12, 1)]"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(title_word_to_id.values()).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 3),\n",
       " ('cell', 4),\n",
       " ('cycle.', 5),\n",
       " ('life', 6),\n",
       " ('of', 7),\n",
       " ('just', 8),\n",
       " ('like', 9),\n",
       " ('an', 10),\n",
       " ('organism', 11),\n",
       " ('...', 12)]"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import islice\n",
    "\n",
    "def take(n, iterable):\n",
    "    \"Return first n items of the iterable as a list\"\n",
    "    return list(islice(iterable, n))\n",
    "\n",
    "take(10, title_word_to_id.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_word_to_id[\"<PAD>\"] = 0 # there is no value this replaces; it just adds a pad\n",
    "title_word_to_id[\"<START>\"] = 1 # BERT doesn't use START tokens so using spaces instead; spaces will be trimmed out\n",
    "title_word_to_id[\"<UNK>\"] = 2 # UNK tokens are good. BERT converts them to ## so it knows it's unknown\n",
    "title_word_to_id[\"<UNUSED>\"] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_id_to_word = {value:key for key, value in title_word_to_id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of title encoding dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up Descriptions Encoding (dictionarize for ALL descriptions (not just train or test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_word_list=[]\n",
    "\n",
    "for i in np.arange(len(new_descriptions)):\n",
    "    description_word_list.append(new_descriptions[i].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_flat_list = []\n",
    "\n",
    "for sublist in description_word_list:\n",
    "    for item in sublist:\n",
    "        description_flat_list.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_word_list = description_flat_list.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_words_unique = list(set(description_word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique train titles word count: 8356\n",
      "Unique validation titles word count: 8356\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique train titles word count: {}\\nUnique validation titles word count: {}\".format(len(description_words_unique), len(description_words_unique)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "description_word_to_id = Counter()\n",
    "\n",
    "for word in description_word_list:\n",
    "    description_word_to_id[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For unique dictionary values of key words\n",
    "description_word_to_id = {k:(i + 3) for i,(k,v) in enumerate(description_word_to_id.items())}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking to make sure the 10 most common words only appear once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 1),\n",
       " (4, 1),\n",
       " (5, 1),\n",
       " (6, 1),\n",
       " (7, 1),\n",
       " (8, 1),\n",
       " (9, 1),\n",
       " (10, 1),\n",
       " (11, 1),\n",
       " (12, 1)]"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(description_word_to_id.values()).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('life', 3),\n",
       " ('science', 4),\n",
       " ('chapter', 5),\n",
       " ('the', 6),\n",
       " ('cell', 7),\n",
       " ('in', 8),\n",
       " ('action.', 9),\n",
       " ('diffusion', 10),\n",
       " ('movement', 11),\n",
       " ('from', 12)]"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import islice\n",
    "\n",
    "def take(n, iterable):\n",
    "    \"Return first n items of the iterable as a list\"\n",
    "    return list(islice(iterable, n))\n",
    "\n",
    "take(10, description_word_to_id.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_word_to_id[\"<PAD>\"] = 0 # there is no value this replaces; it just adds a pad\n",
    "description_word_to_id[\"<START>\"] = 1 # BERT doesn't use START tokens so using spaces instead; spaces will be trimmed out\n",
    "description_word_to_id[\"<UNK>\"] = 2 # UNK tokens are good. BERT converts them to ## so it knows it's unknown\n",
    "description_word_to_id[\"<UNUSED>\"] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_id_to_word = {value:key for key, value in description_word_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(' '.join(description_id_to_word[id] for id in descriptions_test[0])) # This fails because I have words, but this is designed for encoded integer values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of descriptions encoding dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Model setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model # Keras is the new high level API for TensorFlow\n",
    "\n",
    "#max_seq_length = 128  # Your choice here.\n",
    "max_seq_length = 512  # Your choice here.\n",
    "input_word_ids = tf.keras.layers.Input(shape=(max_seq_length), dtype=tf.int32,\n",
    "                                       name=\"input_word_ids\")\n",
    "input_mask = tf.keras.layers.Input(shape=(max_seq_length), dtype=tf.int32,\n",
    "                                   name=\"input_mask\")\n",
    "segment_ids = tf.keras.layers.Input(shape=(max_seq_length), dtype=tf.int32,\n",
    "                                    name=\"segment_ids\")\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2\",\n",
    "                            trainable=True)\n",
    "pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 30522\n"
     ]
    }
   ],
   "source": [
    "import bert\n",
    "import math\n",
    "\n",
    "# Set up tokenizer to generate Tensorflow dataset\n",
    "FullTokenizer = bert.bert_tokenization.FullTokenizer\n",
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = FullTokenizer(vocab_file, do_lower_case)\n",
    "print(\"Vocab size:\", len(tokenizer.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Tokenizer setup for the dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example with BERT tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nickelodeon', 'ce', '##l', ':', 'production', 'art', '|', 'e', '##bay']\n",
      "[20814, 8292, 2140, 1024, 2537, 2396, 1064, 1041, 15907]\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(\"Nickelodeon Cel: Production Art | eBay\")\n",
    "print(tokens)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the CLS (classification start) and SEP  (separate sentences) tokens required for BERT per the BERT paper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_titles_tokens = list(map(lambda titles_train: ['[CLS]'] + tokenizer.tokenize(titles_train)[:510] + ['[SEP]'], titles_train))\n",
    "test_titles_tokens = list(map(lambda titles_test: ['[CLS]'] + tokenizer.tokenize(titles_test)[:510] + ['[SEP]'], titles_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The first method below (lambda) isn't working for the descriptions so need to run the latter functions in stead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_descriptions_tokens = list(map(lambda descriptions_train: ['[CLS]'] + tokenizer.tokenize(descriptions_train)[:510] + ['[SEP]'], descriptions_train))\n",
    "# test_descriptions_tokens = list(map(lambda descriptions_test: ['[CLS]'] + tokenizer.tokenize(descriptions_test)[:510] + ['[SEP]'], descriptions_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_train_tokens = []\n",
    "for desc_train in descriptions_train:\n",
    "    desc_train = ['[CLS]'] + tokenizer.tokenize(desc_train)[:510] + ['[SEP]']\n",
    "    description_train_tokens.append(desc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_test_tokens = []\n",
    "for desc_test in descriptions_test:\n",
    "    desc_test = ['[CLS]'] + tokenizer.tokenize(desc_test)[:510] + ['[SEP]']\n",
    "    description_test_tokens.append(desc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the masks, segment IDs, and tokens for the BERT machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See BERT paper: https://arxiv.org/pdf/1810.04805.pdf\n",
    "# And BERT implementation convert_single_example() at https://github.com/google-research/bert/blob/master/run_classifier.py\n",
    "\n",
    "###############################\n",
    "# RDS  modifications to these #\n",
    "# functions to simply return  #\n",
    "# numpy arrays                #\n",
    "###############################\n",
    "\n",
    "def get_ids(tokens, tokenizer, max_seq_length):\n",
    "    \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n",
    "    return np.array(input_ids)\n",
    "\n",
    "def get_masks(tokens, max_seq_length):\n",
    "    \"\"\"Mask for padding\"\"\"\n",
    "    if len(tokens)>max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    return np.array([1]*len(tokens) + [0] * (max_seq_length - len(tokens)))\n",
    "\n",
    "\n",
    "def get_segments(tokens, max_seq_length):\n",
    "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
    "    if len(tokens)>max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    segments = []\n",
    "    current_segment_id = 0\n",
    "    for token in tokens:\n",
    "        segments.append(current_segment_id)\n",
    "        if token == \"[SEP]\":\n",
    "            current_segment_id = 1\n",
    "    return np.array(segments + [0] * (max_seq_length - len(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 512\n",
    "\n",
    "train_title_tokens_ids = []\n",
    "test_title_tokens_ids = []\n",
    "\n",
    "train_description_tokens_ids = []\n",
    "test_description_tokens_ids = []\n",
    "\n",
    "for i in np.arange(0, len(train_titles_tokens)):\n",
    "    this_train_title_token_id = get_ids(train_titles_tokens[i], tokenizer, max_seq_length=max_seq_length)\n",
    "    train_title_tokens_ids.append(this_train_title_token_id)\n",
    "\n",
    "for i in np.arange(0, len(test_titles_tokens)):\n",
    "    this_test_title_token_id = get_ids(test_titles_tokens[i], tokenizer, max_seq_length=max_seq_length)\n",
    "    test_title_tokens_ids.append(this_test_title_token_id)\n",
    "\n",
    "for i in np.arange(0, len(description_train_tokens)):\n",
    "    this_train_description_token_id = get_ids(description_train_tokens[i], tokenizer, max_seq_length=max_seq_length)\n",
    "    train_description_tokens_ids.append(this_train_description_token_id)\n",
    "\n",
    "for i in np.arange(0, len(description_test_tokens)):\n",
    "    this_test_description_token_id = get_ids(description_test_tokens[i], tokenizer, max_seq_length=max_seq_length)\n",
    "    test_description_tokens_ids.append(this_test_description_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(751, 512)"
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(tokens_ids).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input token IDs dimensions: (751, 512)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Input token IDs dimensions: {}\\n'.format(pd.DataFrame(tokens_ids).shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input token IDs (NOT truncated):\n",
      "\n",
      " [  101  5532 16012  4168  1037 15764  5009  2000  2049  4176  1998  4264\n",
      "   102     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "print(\"Input token IDs (NOT truncated):\\n\\n\", tokens_ids[3][0:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Create text masks for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 512\n",
    "    \n",
    "train_title_tokens_masks = []\n",
    "test_title_tokens_masks = []\n",
    "\n",
    "train_description_tokens_masks = []\n",
    "test_description_tokens_masks = []\n",
    "\n",
    "for i in np.arange(0, len(train_titles_tokens)):\n",
    "    this_train_title_token_mask = get_masks(train_titles_tokens[i], max_seq_length=max_seq_length)\n",
    "    train_title_tokens_masks.append(this_train_title_token_mask)\n",
    "\n",
    "for i in np.arange(0, len(test_titles_tokens)):\n",
    "    this_test_title_token_mask = get_masks(test_titles_tokens[i], max_seq_length=max_seq_length)\n",
    "    test_title_tokens_masks.append(this_test_title_token_mask)\n",
    "\n",
    "for i in np.arange(0, len(description_train_tokens)):\n",
    "    this_train_description_token_mask = get_masks(description_train_tokens[i], max_seq_length=max_seq_length)\n",
    "    train_description_tokens_masks.append(this_train_description_token_mask)\n",
    "\n",
    "for i in np.arange(0, len(description_test_tokens)):\n",
    "    this_test_description_token_mask = get_masks(description_test_tokens[i], max_seq_length=max_seq_length)\n",
    "    test_description_tokens_masks.append(this_test_description_token_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training input masks for title:\n",
      "\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"Training input masks for first title:\\n\\n\", train_title_tokens_masks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training input masks for description:\n",
      "\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"Training input masks for first description:\\n\\n\", train_description_tokens_masks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Create text segments for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 512\n",
    "\n",
    "train_title_tokens_segs = []\n",
    "test_title_tokens_segs = []\n",
    "\n",
    "train_description_tokens_segs = []\n",
    "test_description_tokens_segs = []\n",
    "\n",
    "input_seg = []\n",
    "for i in np.arange(0, len(train_titles_tokens)):\n",
    "    this_train_title_token_seg = get_segments(train_titles_tokens[i], max_seq_length=max_seq_length)\n",
    "    train_title_tokens_segs.append(this_train_title_token_seg)\n",
    "\n",
    "for i in np.arange(0, len(test_titles_tokens)):\n",
    "    this_test_title_token_seg = get_segments(test_titles_tokens[i], max_seq_length=max_seq_length)\n",
    "    test_title_tokens_segs.append(this_test_title_token_seg)\n",
    "\n",
    "for i in np.arange(0, len(description_train_tokens)):\n",
    "    this_train_description_token_seg = get_segments(description_train_tokens[i], max_seq_length=max_seq_length)\n",
    "    train_description_tokens_segs.append(this_train_description_token_seg)\n",
    "\n",
    "for i in np.arange(0, len(description_test_tokens)):\n",
    "    this_test_description_token_seg = get_segments(description_test_tokens[i], max_seq_length=max_seq_length)\n",
    "    test_description_tokens_segs.append(this_test_description_token_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training input segments for first movie review:\n",
      "\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"Training input segments for first title:\\n\\n\", train_title_tokens_segs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training input segments for first description:\n",
      "\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"Training input segments for first description:\\n\\n\", train_description_tokens_segs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of BERT Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BELOW IS A WIP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 500  # Your choice here.\n",
    "input_word_ids = tf.keras.layers.Input(shape=(max_seq_length), dtype=tf.int32,\n",
    "                                       name=\"input_word_ids\")\n",
    "input_mask = tf.keras.layers.Input(shape=(max_seq_length), dtype=tf.int32,\n",
    "                                   name=\"input_mask\")\n",
    "segment_ids = tf.keras.layers.Input(shape=(max_seq_length), dtype=tf.int32,\n",
    "                                    name=\"segment_ids\")\n",
    "pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "classifier1 = tf.keras.layers.Dense(100,activation='sigmoid')(pooled_output)\n",
    "classifier2 = tf.keras.layers.Dense(1,activation='sigmoid')(classifier1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=[classifier2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_word_ids (InputLayer)     [(None, 500)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mask (InputLayer)         [(None, 500)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 500)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_1 (KerasLayer)      multiple             109482241   input_word_ids[0][0]             \n",
      "                                                                 input_mask[0][0]                 \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 100)          76900       keras_layer_1[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            101         dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 109,559,242\n",
      "Trainable params: 109,559,241\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import graphviz\n",
    "import pydot\n",
    "\n",
    "model.summary() # second pass with 500 sequence length\n",
    "tf.keras.utils.plot_model(model, show_shapes=True, dpi=48)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The above will need to be done in a tensorflow 2.1.0 environment. After everything is done and it is time to run the BERT, there will need to be another Python script that runs the model in M2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
